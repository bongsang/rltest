{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "p1_DQN_LunarLander.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2532db00"
      },
      "source": [
        "# Deep Q-Network (DQN) on LunarLander-v2\n",
        "\n",
        "> In this post, We will take a hands-on-lab of Simple Deep Q-Network (DQN) on openAI LunarLander-v2 environment. "
      ],
      "id": "2532db00"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a90b5d5d"
      },
      "source": [
        "## Deep Q-Network (DQN)\n",
        "---\n",
        "In this notebook, you will implement a DQN agent with OpenAI Gym's LunarLander-v2 environment.\n",
        "\n",
        "### Import the Necessary Packages"
      ],
      "id": "a90b5d5d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9dcd84a",
        "outputId": "a14a918d-79d8-47a2-a4f8-d50d9d7379dd"
      },
      "source": [
        "!pip install gym[Box_2D] Box2D box2d-py torch"
      ],
      "id": "e9dcd84a",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: Box2D in /usr/local/lib/python3.7/dist-packages (2.3.10)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c53353c"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import base64, io\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display \n",
        "import glob"
      ],
      "id": "5c53353c",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da041e9a"
      },
      "source": [
        "### Instantiate the Environment and Agent\n",
        "\n",
        "Initialize the environment."
      ],
      "id": "da041e9a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f11b8f5a",
        "outputId": "b7f36e87-d522-426b-8701-557035ffde61"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env.seed(0)\n",
        "print('State shape: ', env.observation_space.shape)\n",
        "print('Number of actions: ', env.action_space.n)"
      ],
      "id": "f11b8f5a",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (8,)\n",
            "Number of actions:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ad25876"
      },
      "source": [
        "### Define Neural Network Architecture. (3점)\n",
        "\n",
        "Since `LunarLander-v2` environment is sort of simple envs, we don't need complicated architecture. We just need non-linear function approximator that maps from state to action."
      ],
      "id": "3ad25876"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a85e8e5"
      },
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        ############################### TODO: YOUR CODE BELOW ##############################\n",
        "        ### Q Network에 사용될 layers을 정의해주세요                                          ###\n",
        "        ####################################################################################\n",
        "        self.layer1 = nn.Linear(state_size, 128)\n",
        "        self.layer2 = nn.Linear(128, 64)\n",
        "        self.layer3 = nn.Linear(64, 32)\n",
        "        self.layer4 = nn.Linear(32, action_size)\n",
        "        ################################# END OF YOUR CODE #################################\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        \n",
        "        ############################### TODO: YOUR CODE BELOW ##############################\n",
        "        ###  Forward Pass를 구현해주세요                                                    ###\n",
        "        ####################################################################################        \n",
        "        x = F.relu(self.layer1(state))        \n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.relu(self.layer3(x))\n",
        "        x = self.layer4(x)\n",
        "        ################################# END OF YOUR CODE #################################\n",
        "        \n",
        "        return x"
      ],
      "id": "1a85e8e5",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6e8108e"
      },
      "source": [
        "### Define some hyperparameter (1점)\n",
        "\n",
        "> 참고로 초매개변수 선택에는 답이 정해져있지 않습니다. 여러 케이스에 대해서 테스트해보고 최적의 조합을 찾아보기 바랍니다."
      ],
      "id": "d6e8108e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa5b3f99"
      },
      "source": [
        "############################### TODO: YOUR CODE BELOW ##############################\n",
        "### 초매개변수를 정의해주세요                                                    ###\n",
        "####################################################################################\n",
        "BUFFER_SIZE = 100000  # replay buffer size\n",
        "BATCH_SIZE = 128         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 0.001              # for soft update of target parameters\n",
        "LR = 0.0001               # learning rate \n",
        "UPDATE_EVERY = 10        # how often to update the network\n",
        "################################# END OF YOUR CODE #################################"
      ],
      "id": "aa5b3f99",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24a52d23"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "24a52d23",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46b012d7"
      },
      "source": [
        "### Define Replay Buffer (4점)"
      ],
      "id": "46b012d7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3adae118"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        ############################### TODO: YOUR CODE BELOW ##############################\n",
        "        ### Hint1: 실제 torch로 데이터를 처리하기 위해서는 numpy array를 torch tensor로 변환해줘야 합니다. torch.from_numpy()를 참고하기 바랍니다.\n",
        "        ### Hint2: 경험에서 추출된 정보를 한번에 사용하기 위해서는 주어진 데이터를 쌓아야 합니다. np.vstack()를 참고하기 바랍니다.\n",
        "        ### Hint3: 각 정보는 형변환이 되어야 합니다. \n",
        "        ####################################################################################\n",
        "        \n",
        "        # Convert to torch tensors\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        \n",
        "        # Convert done from boolean to int\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)        \n",
        "        \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "        ################################# END OF YOUR CODE #################################\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "id": "3adae118",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f993d953"
      },
      "source": [
        "### Define Agent (6점)"
      ],
      "id": "f993d953"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bb41f11"
      },
      "source": [
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "        ############################### TODO: YOUR CODE BELOW ##############################\n",
        "        ### optimizer를 정의해주세요\n",
        "        ### Hint1: optimizer가 최적화해야 할 weight을 생각해보기 바랍니다.\n",
        "        ####################################################################################\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "        ################################# END OF YOUR CODE #################################\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "        self.t_step = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \"\"\"Returns actions for given state as per current policy.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state (array_like): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        \n",
        "        \n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            ############################### TODO: YOUR CODE BELOW ##############################\n",
        "            ## TODO: Action을 q network로부터 뽑아내세요.\n",
        "            ### Hint: 우리가 행동을 뽑아내는 신경망은 어떤 것인지 생각해보기 바랍니다.\n",
        "            ####################################################################################\n",
        "            action_values = self.qnetwork_local(state)\n",
        "            ################################# END OF YOUR CODE #################################\n",
        "            \n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        ############################### TODO: YOUR CODE BELOW ##############################\n",
        "        ## 입실론-그리디 탐색을 구현하세요.\n",
        "        ####################################################################################\n",
        "        if random.random() > eps:\n",
        "            ## TODO: 임의의 확률이 eps보다 클때 action 선택 방법\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            ## TODO: 임의의 확률이 eps보다 작을때 action 선택 방법\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "        ################################# END OF YOUR CODE #################################\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        # Obtain random minibatch of tuples from D\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ## Compute and minimize the loss\n",
        "        ### Extract next maximum estimated value from target network\n",
        "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        ############################### TODO: YOUR CODE BELOW ##############################\n",
        "        ## bellman equation을 구현해주세요\n",
        "        ####################################################################################        \n",
        "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
        "        ################################# END OF YOUR CODE #################################\n",
        "        \n",
        "        ### Calculate expected value from local network\n",
        "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "        \n",
        "        ############################### TODO: YOUR CODE BELOW ##############################\n",
        "        ## mse를 사용해서 loss를 계산하기 바랍니다.\n",
        "        ####################################################################################\n",
        "        loss = F.mse_loss(q_expected, q_targets)\n",
        "        ################################# END OF YOUR CODE #################################\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ------------------- update target network ------------------- #\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            local_model (PyTorch model): weights will be copied from\n",
        "            target_model (PyTorch model): weights will be copied to\n",
        "            tau (float): interpolation parameter \n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
      ],
      "id": "8bb41f11",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9580d0f3"
      },
      "source": [
        "### Training Process (5점)"
      ],
      "id": "9580d0f3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c6ae9df",
        "outputId": "ad0f3838-6aee-4c5f-a2fc-77c6da44f26b"
      },
      "source": [
        "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    \"\"\"Deep Q-Learning.\n",
        "    \n",
        "    Params\n",
        "    ======\n",
        "        n_episodes (int): maximum number of training episodes\n",
        "        max_t (int): maximum number of timesteps per episode\n",
        "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "        eps_end (float): minimum value of epsilon\n",
        "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "    \"\"\"\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)  # last 100 scores\n",
        "    eps = eps_start                    # initialize epsilon\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            ############################### TODO: YOUR CODE BELOW ##############################\n",
        "            ## STEP1: action을 agent로부터 뽑습니다.\n",
        "            action = agent.act(state, eps)\n",
        "            \n",
        "            ## STEP2: env.step()을 사용해서 원하는 정보를 뽑습니다.\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            ## STEP3: 뽑아낸 정보를 바탕으로 agent를 학습시킵니다.\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            \n",
        "            ## STEP4: 다음 상태로 갱신시킵니다\n",
        "            state = next_state\n",
        "            \n",
        "            ## STEP5: score를 정의해주세요.\n",
        "            score += reward\n",
        "            \n",
        "            ################################# END OF YOUR CODE #################################\n",
        "            if done:\n",
        "                break \n",
        "        scores_window.append(score)       # save most recent score\n",
        "        scores.append(score)              # save most recent score\n",
        "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=200.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "            break\n",
        "    return scores\n",
        "\n",
        "agent = Agent(state_size=8, action_size=4, seed=0)\n",
        "scores = dqn()"
      ],
      "id": "9c6ae9df",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -223.61\n",
            "Episode 200\tAverage Score: -182.40\n",
            "Episode 300\tAverage Score: -198.34\n",
            "Episode 400\tAverage Score: -187.14\n",
            "Episode 500\tAverage Score: -98.06\n",
            "Episode 600\tAverage Score: -93.79\n",
            "Episode 700\tAverage Score: -75.37\n",
            "Episode 800\tAverage Score: -55.77\n",
            "Episode 900\tAverage Score: -31.08\n",
            "Episode 1000\tAverage Score: -33.53\n",
            "Episode 1100\tAverage Score: -37.08\n",
            "Episode 1200\tAverage Score: -26.77\n",
            "Episode 1300\tAverage Score: -21.28\n",
            "Episode 1400\tAverage Score: 2.21\n",
            "Episode 1500\tAverage Score: 36.55\n",
            "Episode 1600\tAverage Score: 88.78\n",
            "Episode 1700\tAverage Score: 126.92\n",
            "Episode 1800\tAverage Score: 193.98\n",
            "Episode 1853\tAverage Score: 200.88\n",
            "Environment solved in 1753 episodes!\tAverage Score: 200.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "756e7735"
      },
      "source": [
        "### Plot the learning progress (1점)"
      ],
      "id": "756e7735"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0258f303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "5df3aa43-8fb1-435c-b3c8-632ddb10c65e"
      },
      "source": [
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "id": "0258f303",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba7e5fec"
      },
      "source": [
        "### Animate it with Video"
      ],
      "id": "ba7e5fec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5de6b9d6"
      },
      "source": [
        "def show_video(env_name):\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env\n",
        "\n",
        "def gen_wrapped_env(env_name):\n",
        "    return wrap_env(gym.make(env_name))\n",
        "        \n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gen_wrapped_env(env_name)\n",
        "    agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        frame = env.render(mode='rgb_array')\n",
        "        \n",
        "        action = agent.act(state)\n",
        "\n",
        "        state, reward, done, _ = env.step(action)        \n",
        "    env.close()"
      ],
      "id": "5de6b9d6",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c08b0dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "23211e85-ef58-4d34-c054-4cb4c30d70be"
      },
      "source": [
        "agent = Agent(state_size=8, action_size=4, seed=0)\n",
        "show_video_of_model(agent, 'LunarLander-v2')"
      ],
      "id": "c08b0dd9",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b4fbd53cdbbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshow_video_of_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-65e1c083bf53>\u001b[0m in \u001b[0;36mshow_video_of_model\u001b[0;34m(agent, env_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_wrapped_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVIEWPORT_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVIEWPORT_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59007b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "803ca0ee-137f-4fed-c040-657ad931d25f"
      },
      "source": [
        "show_video('LunarLander-v2')"
      ],
      "id": "59007b73",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58eaba28"
      },
      "source": [
        ""
      ],
      "id": "58eaba28",
      "execution_count": null,
      "outputs": []
    }
  ]
}